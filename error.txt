Computing keyword embeddings...
Initializing BERT model...
BERT model initialized. Time taken: 0.0 seconds.
Keyword embeddings computed. Time taken: 5.824897289276123 seconds.
Preprocessing comments and summarizing if necessary...
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0010001659393310547 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0010008811950683594 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.0 seconds.
Preprocessing text...
Preprocessing text completed. Time taken: 0.001009225845336914 seconds.
Start Summarizing text...
Start Summarization Pipeline text...
Time taken to initialize summarization pipeline: 4.5888307094573975
Starting summarization of 16 texts...
  0%|                                                                                                                                                                                                               | 0/16 [00:00<?, ?it/sT 
oken indices sequence length is longer than the specified maximum sequence length for this model (1995 > 1024). Running this sequence through the model will result in indexing errors
 12%|████████████████████████▉                                                                                                                                                                              | 2/16 [00:28<03:20, 14.29s/it]Your max_length is set to 100, but your input_length is only 15. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)
 19%|█████████████████████████████████████▎                                                                                                                                                                 | 3/16 [00:44<03:14, 14.99s/it]Your max_length is set to 100, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [07:24<00:00, 27.80s/it]
Summarization completed.
Time taken to process summarization: 449.43202328681946
2023-07-28 16:37:37.412 Uncaught app exception
Traceback (most recent call last):
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\storage\in_memory_cache_storage_wrapper.py", line 87, in get
    entry_bytes = self._read_from_mem_cache(key)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\storage\in_memory_cache_storage_wrapper.py", line 137, in _read_from_mem_cache
    raise CacheStorageKeyNotFoundError("Key not found in mem cache")
streamlit.runtime.caching.storage.cache_storage_protocol.CacheStorageKeyNotFoundError: Key not found in mem cache

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\cache_data_api.py", line 634, in read_result
    pickled_entry = self.storage.get(key)
                    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\storage\in_memory_cache_storage_wrapper.py", line 89, in get
    entry_bytes = self._persist_storage.get(key)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\storage\local_disk_cache_storage.py", line 155, in get
    raise CacheStorageKeyNotFoundError(
streamlit.runtime.caching.storage.cache_storage_protocol.CacheStorageKeyNotFoundError: Local disk cache storage is disabled (persist=None)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\cache_utils.py", line 263, in _get_or_create_cached_value
    cached_result = cache.read_result(value_key)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\cache_data_api.py", line 636, in read_result
    raise CacheKeyNotFoundError(str(e)) from e
streamlit.runtime.caching.cache_errors.CacheKeyNotFoundError: Local disk cache storage is disabled (persist=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\storage\in_memory_cache_storage_wrapper.py", line 87, in get
    entry_bytes = self._read_from_mem_cache(key)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\storage\in_memory_cache_storage_wrapper.py", line 137, in _read_from_mem_cache
    raise CacheStorageKeyNotFoundError("Key not found in mem cache")
streamlit.runtime.caching.storage.cache_storage_protocol.CacheStorageKeyNotFoundError: Key not found in mem cache

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\cache_data_api.py", line 634, in read_result
    pickled_entry = self.storage.get(key)
                    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\storage\in_memory_cache_storage_wrapper.py", line 89, in get
    entry_bytes = self._persist_storage.get(key)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\storage\local_disk_cache_storage.py", line 155, in get
    raise CacheStorageKeyNotFoundError(
streamlit.runtime.caching.storage.cache_storage_protocol.CacheStorageKeyNotFoundError: Local disk cache storage is disabled (persist=None)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\cache_utils.py", line 311, in _handle_cache_miss
    cached_result = cache.read_result(value_key)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\cache_data_api.py", line 636, in read_result
    raise CacheKeyNotFoundError(str(e)) from e
streamlit.runtime.caching.cache_errors.CacheKeyNotFoundError: Local disk cache storage is disabled (persist=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python311\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 552, in _run_script
    exec(code, module.__dict__)
  File "C:\Users\m.berenji\Desktop\To Move\git\NPS Script\transcript_categories\batch_summary_transcript_v1broken.py", line 359, in <module>
    trends_data = process_feedback_data(feedback_data, comment_column, date_column, categories, similarity_threshold)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\cache_utils.py", line 211, in wrapper
    return cached_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\cache_utils.py", line 240, in __call__
    return self._get_or_create_cached_value(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\cache_utils.py", line 266, in _get_or_create_cached_value
    return self._handle_cache_miss(cache, value_key, func_args, func_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\streamlit\runtime\caching\cache_utils.py", line 320, in _handle_cache_miss
    computed_value = self._info.func(*func_args, **func_kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\m.berenji\Desktop\To Move\git\NPS Script\transcript_categories\batch_summary_transcript_v1broken.py", line 266, in process_feedback_data
    long_comments_summaries = pd.DataFrame({
                              ^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\pandas\core\frame.py", line 663, in __init__
    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\pandas\core\internals\construction.py", line 493, in dict_to_mgr
    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\pandas\core\internals\construction.py", line 118, in arrays_to_mgr
    index = _extract_index(arrays)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python311\Lib\site-packages\pandas\core\internals\construction.py", line 666, in _extract_index
    raise ValueError("All arrays must be of the same length")
ValueError: All arrays must be of the same length

def summarize_text(texts, max_length=100, min_length=50, max_tokens=1024, max_chunk_len=128, min_word_count=80):
    start_time = time.time()
    print("Start Summarizing text...")
    # Initialize the summarization pipeline
    summarization_pipeline = get_summarization_pipeline()

    # Initialize a list to store the summaries
    all_summaries = []

    # Initialize the current chunk
    current_chunk = []
    current_chunk_tokens = 0

    total_texts = len(texts)  # total number of texts
    print(f"Starting summarization of {total_texts} texts...")

    # Initialize progress bar
    pbar = tqdm(total=total_texts)

    # Iterate over the texts
    for idx, text in enumerate(texts):
        # Skip summarizing the text if the word count is below the threshold
        if len(text.split()) <= min_word_count:
            all_summaries.append(text)
            pbar.update(1)
            continue

        tokens = len(summarization_pipeline.tokenizer(text)["input_ids"])  # simple whitespace tokenization

        # If a single text is too long, split it into multiple parts
        if tokens > max_tokens:
            text_parts = textwrap.wrap(text, max_tokens)  # split the text into parts
            for text_part in text_parts:
                tokens = len(summarization_pipeline.tokenizer(text_part)["input_ids"])  # compute the number of tokens for each part
                if current_chunk_tokens + tokens > max_tokens or len(current_chunk) == max_chunk_len:  # check if adding this text part exceeds the token limit
                    summaries = summarization_pipeline(current_chunk, max_length=max_length, min_length=min_length, do_sample=False)
                    all_summaries.extend([summary['summary_text'] for summary in summaries])
                    current_chunk = [text_part]
                    current_chunk_tokens = tokens
                else:
                    current_chunk.append(text_part)
                    current_chunk_tokens += tokens
        else:
            if current_chunk_tokens + tokens > max_tokens or len(current_chunk) == max_chunk_len:  # check if adding this text exceeds the token limit
                summaries = summarization_pipeline(current_chunk, max_length=max_length, min_length=min_length, do_sample=False)
                all_summaries.extend([summary['summary_text'] for summary in summaries])
                current_chunk = [text]
                current_chunk_tokens = tokens
            else:
                current_chunk.append(text)
                current_chunk_tokens += tokens

        # Update the progress bar
        pbar.update(1)

    # Process the last chunk if it's not empty
    if current_chunk:
        summaries = summarization_pipeline(current_chunk, max_length=max_length, min_length=min_length, do_sample=False)
        all_summaries.extend([summary['summary_text'] for summary in summaries])
